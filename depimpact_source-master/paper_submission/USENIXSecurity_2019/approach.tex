\section{The Design of \tool}
\label{sec:approach}

In this section, we present the three phases and the components of \tool in detail.

\input{tables/syscalls.tex}
\subsection{Phase I: Dependency Graph Generation}
\label{subsec:graph-generation}

\myparatight{System Auditing}
\tool leverages system auditing tools~\cite{auditd,etw,dtrace,sysdig} to collect information about system calls from the kernel, and then parses the collected events to build a global dependency graph.
\tool focuses on three types of system events: 
(i) file access, 
(ii) processes creation and destruction, and
(iii) network access.
\cref{tab:events} shows the representative system calls (in Linux) processed by \tool.
Particularly, for a process entity, we use the process name and PID as its unique identifier.
For a file entity, we use the absolute path as its unique identifier. 
For a network 
%socket 
connection entity, as processes usually communicate with some servers using different network connections but with the same IPs and ports, treating these connections differently greatly increases the amount of data we trace and such granularity is not required in most of the cases~\cite{liu2018priotracker,gao2018aiql,gao2018saql}. Thus, we use 5-tuple (\emph{$\langle$srcip, srcport, dstip, dstport, protocol$\rangle$)} as a network 
connection's
%socket's 
unique identifier. 
Failing to distinguish different entities causes problems in relating events to entities and tracking
%, especially for tracking 
the dependencies of events.
For each system call, \tool extracts 
%a set of 
the security-related attributes of entities (\cref{tab:entity-attributes}) and events (\cref{tab:event-attributes}).
\tool filters out failed system calls, which could cause false dependencies among events.


\myparatight{Causality Analysis}
Given a POI event $e_s$, \tool applies causality analysis (\cref{subsec:causality-analysis}) to produce a dependency graph $G_d$, as shown in \cref{alg:backTrack}\footnote{Forward causality analysis can be implemented in a similar way.}.
\cref{alg:backTrack} adds $e_s$ to a queue (Line 2), and repeats the process of finding eligible incoming edges of the edges (\ie incoming edges of the source nodes of edges) in the queue (Lines 3-9) until the queue is empty. The output is a dependency graph that only contains relevant system entities and events w.r.t. the given POI event.

\input{BackTrackPseudo}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Phase II: Graph Preprocessing}
\label{subsec:graph-preprocessing}

\tool performs graph preprocessing to transform the dependency graph from a directed multigraph to a simple directed graph with no parallel edges.

\myparatight{Edge Merge}
The dependency graph produced by causality analysis often has many edges between the same pair of nodes~\cite{reduction}.
%For example, in \cref{before}, the \incode{wget} process has many edges going to and coming from a network socket, and the \incode{unzip} process also has many edges going to and coming from the file \incode{INSTALL}. Both of these pairs are shown in red dotted rectangles in the \cref{before}.\pgao{replace with new text}
The reason for generating these excessive edges is that the OS typically finishes a read/write task (\eg file read/write) by distributing the data to multiple system calls and each system call processes only a portion of the data.
% Although these edges preserves causal dependency~\cite{backtracking,reduction},
% they create complication in propagating reputations: 
% the reputation of the same node will be propagated multiple times.
Inspired by the recent work that proposes Causality Preserved Reduction (CPR)~\cite{reduction} for dependency graph reduction, \tool merges the edges between two nodes.
As shown in the study~\cite{reduction}, CPR does not work well for processes that have many interleaved read and write system calls, which introduces excessive causality.
As such, \tool adopts a more aggressive approach: for edges between two nodes that represent the same system call, 
%(\eg file reads from \incode{read} or network reads from \incode{recvfrom}), 
\tool will merge them into one edge if the time differences of these edges are smaller than a given time threshold. 
Empirically, we set the time threshold as 10 seconds, 
%since 10 seconds 
which is large enough for most processes to finish file transfers and network communications in modern computers. 
Since such merge is performed after the dependency graph generation, all the dependencies are still preserved but only the time windows of certain edges are merged. 


\myparatight{Node Split}
After the edge merge, the dependency graph may still have parallel edges (\ie edges indicating read or write from different system calls).
For example, a process may receive data from a network socket via both \incode{read} and \incode{recvfrom}.
% Although we merge edges within 10 seconds, it is possible that there are still parallel edges because of some long-running file transfers.
These parallel edges create complications for weight computation: for a node, some of its incoming edges' time windows may violate the causal dependencies on the outgoing edges.
% , making it difficult to represent the weights of all the edges using a transition matrix.

To address the problem, 
\tool first enumerates all the pairs of nodes that have parallel edges.
For a pair of nodes $(u,v)$ that have parallel edges from $u$ to $v$,
\tool splits $u$ into multiple copies and assigns each copy to one parallel edge, so that each copy of $u$ only has one outgoing edge to $v$. The copies of $u$ also inherit all $u$'s incoming edges that have causal dependencies on the $u$'s outgoing edges.
The output 
% after the node split 
is a simple directed dependency graph without parallel edges.
%After the node split, the dependency graph becomes a simple directed graph without parallel edges, which allows \tool to use a transition matrix to present the weights of edges.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Phase II: Feature Extraction}
\label{subsec:feature-extraction}
For each edge, \tool extracts three novel discriminative features that model its importance w.r.t. the POI event.


\begin{itemize}[noitemsep, topsep=1pt, partopsep=1pt, listparindent=\parindent, leftmargin=*]

\item \textbf{Relative Data Amount Difference:}
For an edge $e(u, v)$, we measure the distance between the size of data processed by the system call and the size of POI entity.
%the data amount associated with this event edge and the size of the target system entity in the POI event $e_s$.
The intuition is that the smaller the distance is, the more important this edge is to the data flow in the POI event.
%the closer the data amount of an event is to the size of the target system entity, the more likely this event edge is related to the POI event, and hence should be associated with a higher feature value. Thus, 
%We define the \emph{data amount difference feature} $f_{D(e)}$ for the edge $e$ as:
\begin{equation}
\label{eq:data-feature}
    f_{D(e)} = 1/(\mid s_{e} - s_{e_s}\mid + \alpha),
\end{equation}
\cref{eq:data-feature} gives the \emph{relative data amount difference feature},
where $s_{e}$ and $s_{e_s}$ represent the data amount associated with the event edge $e$ and the POI event $e_s$.
% In (\ref{eq:data-feature}), $f_{D(e)}$ is a positive real number. The smaller the data amount difference $\mid s_{e} - s_{e_s}\mid$ get, the larger the value of $f_{D(e)}$ becomes, and hence the more trust the source node $u$ puts on the destination node $v$ in the data dimension.
Note that
%in (\ref{eq:data-feature}), 
we use a small positive number $\alpha$ to handle the special case when $e$ is the POI event. Thus, the POI event will have the highest data amount difference feature value $f_{D(e_s)} = 1/\alpha$. Empirically, we set $\alpha = 1\mathrm{e}{-4}$.


\item \textbf{Relative Time difference:}
For an edge $e(u,v)$, we measure the distance between its end time $te(e)$ and the end time of the POI event $te(e_s)$.
The intuition is that the event that occurred closer to the POI event is more temporally related to the POI event.
%, and hence should be associated with a higher feature value. 
%Thus, we define the \emph{time difference feature} $f_{T(e)}$ for the edge $e$ as:
\begin{equation}
\label{eq:time-feature}
    f_{T(e)} = \ln(1 + 1/\mid t_{e} - t_{e_s}\mid),
\end{equation}
\cref{eq:time-feature} gives the \emph{relative time difference} feature, where $t_{(e)}$ and $t_{e_s}$ represent timestamp values (we use the event end time) 
%(in seconds) 
of the event edge $e$ and the POI event $e_s$. 
To handle the special case when $e$ is the POI event (\ie $\mid t_{e} - t_{e_s}\mid = 0$), we use one tenth of the minimal time unit (nanosecond) in the audit logging framework (\ie $1\mathrm{e}{-10}$) to compute its feature: $f_{T(e_s)} = ln(1 + 1\mathrm{e}{10})$. This ensures that the POI event has the highest feature value.

%In (\ref{eq:time-feature}), $f_{T(e)}$ is a positive real number. The smaller the time difference $\mid t_e - t_{e_s}\mid$ gets, the larger the value of $f_{T(e)}$ becomes, and hence the more trust the source node $u$ puts on the destination node $v$ in the temporal dimension.
%Special action needs to be taken when the event edge $e$ is actually the POI event, and hence $\mid t_{e} - t_{e_s}\mid = 0$. To handle such a case, we specify that the minimal time difference to be one tenth of the minimal unit (\ie nanosecond) for the timestamp field in the audit logging framework (\ie $1\mathrm{e}{-10}$), and use this minimal value when computing the time difference feature of the POI event. By doing so, the POI event will have the highest time difference feature value 
%$f_{T(e_s)} = ln(1 + 1\mathrm{e}{-10})$.



\item \textbf{Concentration Degree:}
One important category of non-critical edges that often appear in a causal graph are events that access system libraries~\cite{reduction,reduction2}. These edges are often associated with considerable data amount and occur at various timestamps, and hence using only $f_{T(e)}$ and $f_{D(e)}$ is less effective in distinguishing critical edges from them.
To address this challenge, we observe that most system library nodes are source nodes in the corresponding edges and do not have any incoming edges.
Another category of non-critical edges are events that involve long-running processes as source nodes, which often have few incoming edges but many outgoing edges.
%Also, many long-running processes have few incoming edges but many outgoing edges, and thus the weight of each outgoing edge should be smaller.
Based on this observation, we define the \emph{concentration degree} for the edge $e(u, v)$ as:
\begin{equation}
    \label{eq:structure-feature}
    f_{C(e)} = InDegree(u)/OutDegree(u),
\end{equation}
where $InDegree(u)$ and $OutDegree(u)$ represent the in-degree and out-degree of the source node $u$ in $e(u,v)$.
For seed nodes, since they are very important in initiating the reputation propagation, we set their concentration degree to be $1$.
This feature helps smooth out the impacts of system libraries with no incoming edges and long-running processes with many outgoing edges.

% In (\ref{eq:structure-feature}), $f_{S(e)}$ is a non-negative real number. The larger the $InDegree(u)$ gets, the more trust the source node $u$ receives from is parent nodes, and hence the larger the value of $f_{S(e)}$ becomes. Similarly, the larger the $OutDegree(u)$ gets, the more number of child nodes the source node $u$ needs to puts its trust on, and hence the smaller the value of $f_{S(e)}$ becomes. 

%Note that if $u$ is a system library node, $InDegree(u)$ is likely to be zero, and hence $f_{D(e)} = 0$. 
\end{itemize}




\myparatight{Local Feature Normalization}
% \label{subsubsec:feature-normalization}
Before using the three features to compute edge weights, it is often a good practice to scale them in the same range~\cite{mlbook,friedman2001elements}.
Global feature scaling using standard methods such as range normalization and standardization does not 
%intuitively
make much sense in our context, since a node is only affected by its parents but not by its children or siblings.
Recognizing this, \tool adopts a \emph{local feature scaling} scheme: for an edge $e(u, v)$, \tool \emph{locally normalizes} its each feature by the sum of corresponding features of all incoming edges of the sink node $v$. 
This scheme enables the three features of all $v$'s incoming edges to be compared on the same scale.
%The higher the normalized feature is, the more impact its corresponding source node has on the sink node $v$.

\eat{
Formally, for an edge $e(u,v)$, its feature $f_{(e)}$ is locally normalized according to the following equation:
\begin{equation}
    \label{eq:local-feature-normalization}
    f_{(e)} = f_{(e)}/\sum_{e' \in IncomingEdge(v)} f_{(e')}
\end{equation}
where $IncomingEdge(v)$ represents the set of incoming edges of node $v$.

We use (\ref{eq:local-feature-normalization}) to normalize the three features $f_{T(e)}$, $f_{D(e)}$, and $f_{S(e)}$. After normalization, each feature is scaled to a value in $[0, 1]$.
}



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\subsection{Phase II: Weight Computation}
\label{subsec:weight-computation}


For each edge, \tool computes a discriminative weight using its three normalized features. The 
%edge 
weight models the aggregated importance of the event edge w.r.t. the POI event in the attack sequence reconstruction, so that critical edges can be easily distinguished from non-critical edges.

One approach to compute a weight score is to adopt a classification-based approach to train a binary classifier using the three features and output a probability score.
Though this approach has demonstrated its applicability for several domains~\cite{gao2018sybilfuse,wang2018graph}, it faces significant limitations in our problem context.
To achieve high classification accuracy, supervised learning-based approaches often require large amount of training data and the training data and the test data come from the same distribution~\cite{mlbook,friedman2001elements}.
However, in our context, features are extracted w.r.t. the specific POI event, and thus the model learned for one type of attack with a specific POI can hardly generalize to other types of attacks with different POIs. 
The highly imbalanced classes (\ie the very few number of critical edges as compared to non-critical edges) further impedes the model generalization.

% dimensionality reduction + supervised by clustering
Recognizing such limitations, \tool leverages ideas from dimensionality reduction and discriminant analysis~\cite{friedman2001elements}.
Specifically, \tool employs a novel \emph{discriminative local feature projection mechanism} based on an extended version of Linear Discriminant Analysis (LDA)~\cite{Mika99fisherdiscriminant}, and computes a projection vector to project the three-dimensional feature vector to a one-dimensional weight, while ensuring that the projected weights of critical edges and non-critical edges are maximally separated.
We next present the weight computation mechanism (as shown in \cref{alg:weightComputation}) in detail.

\input{WeightCalculationPseudo.tex}


\eat{
Linear projection is known for its high interpretability and low computational cost~\cite{friedman2001elements}.
Formally, for an edge $e$, its weight $W_e$ is computed by projecting its feature vector $\mathbf{f}_{(e)} = (f_{T(e)}, f_{D(e)}, f_{C(e)})$ onto a unit vector $\mathbf{w}_{e} = (w_{T(e)}, w_{D(e)}, w_{C(e)})$:
\begin{align}
    W_e   = &\; \mathbf{f}_{e} \boldsymbol{\cdot} \mathbf{w}_{e} \nonumber \\
                 = &\; w_{T(e)} * f_{T(e)} + w_{D(e)} * f_{D(e)} \nonumber \\
                & + w_{C(e)} * f_{C(e)}\label{eq:projection}
\end{align}
where $\norm{\mathbf{w}_{(e)}} = 1$.
}

\eat{
Different from another popular dimensionality reduction method PCA, which finds the projection vector that maximizes the variance of projected samples, discriminant analysis searches for the projection that maximizes the class separation of projected samples, and linear discriminant analysis (LDA)~\cite{Mika99fisherdiscriminant} the most popular approach.
Algorithm~\ref{alg:weightComputation} shows the detailed steps of weight computation.
}

\eat{
However, we are not able to directly applying standard LDA, due to the following reasons:
(1) LDA requires labeld samples from different classes but we do not have labels for critical edges and non-critical edges;
(2) Our problem context is highly localized since a node is only affected by its parents but not by its children or siblings. Globally computing the projection vector for all edges might lead to serious bias (\cref{subsec:reputation-results});
(3) Standard LDA does not assume the within-class scatter matrix to be singular. However, this may be often in our context since the edges are highly imbalanced.
}


\myparatight{Step 1: Local Edge Clustering}
Due to the localized nature of our problem context and features (\cref{subsec:feature-extraction}), rather than computing a projection vector globally for all edges, 
\tool computes a projection vector \emph{locally} for the incoming edges of each node.
This helps avoid the bias introduced by the large number of irrelevant edges.

In Step 1, for each node, \tool locally clusters all its incoming edges in two groups, which is a prerequisite for discriminant analysis. 
Specifically, \tool adopts Multi-KMeans++ clustering algorithm~\cite{Arthur:2007:KAC:1283383.1283494}.
Based on KMeans, KMeans++ uses a different method for choosing the initial seeds to avoid poor clustering.
%KMeans++/KMeans clustering aims to partition the observations (points) into $k$ clusters such that each observation belongs to the cluster with the nearest center. 
Multi-KMeans++ is a meta algorithm that performs $n$ runs of KMeans++ and then chooses the best clustering that has the lowest distance variance over all clusters.
Empirically, we set $k=2$ (since we want two groups) and $n=20$ (to ensure the best clustering is chosen).


\myparatight{Step 2: Local Feature Projection via Extended LDA}
For each node, after locally clustering its incoming edges in two groups, \tool employs an extended version of Linear Discriminant Analysis (LDA)~\cite{Mika99fisherdiscriminant} to compute a projection vector, and applies the projection vector to the feature vector of each incoming edge to compute an edge weight. 

Formally, for a node $v$, we denote the feature vectors of its $N$ incoming edges as $x^{1}, x^{2}, \dots, x^{N}$, which are clustered in two groups: group $g_1$ contains $N_1$ edges, and group $g_2$ contains $N_2$ edges (\ie $N_1 + N_2 = N$). 
The group mean vectors are $\mu_1 = \frac{1}{N_1}\sum_{x \in g_1} x$ and $\mu_2 = \frac{1}{N_2}\sum_{x\in g_2} x$.
The between-group scatter matrix is defined as $S_b = (\mu_1 - \mu_2)(\mu_1 - \mu_2)^T$.
The within-group scatter matrix is defined as $S_w = \sum_{x\in g_i}(x-\mu_i)(x-\mu_i)^T$.
LDA is a technique 
%in discriminant analysis 
that seeks to reduce dimensionality while preserving as much of the group discriminatory information as possible. Specifically, LDA finds a projection vector $\omega$ that maximizes the following Fisher criterion:

\begin{equation}
    \label{eq:lda-objective}
    J(\omega) = \frac{\omega^TS_b\omega}{\omega^TS_w\omega}
\end{equation}

In order words, LDA looks for the best projection direction such that the projected samples from the same group are close to each other (as enforced by the denominator $\omega^TS_w\omega$), and the projected samples from different groups are far away from each other as possible (as enforced by the numerator $\omega^TS_b\omega$). Solving the optimization problem yields:

%Taking the derivative of $J(w)$ and equate it to zero, we get $S_bw =\lambda S_ww$. If $S_w$ is non-singular, we can convert \cref{eq:lda-objective} to a standard eigenvalue problem $S_w^{-1}S_bw = \lambda w$. 

\begin{equation}
    \label{eq:lda-solution}
    \omega^* = \argmax J(\omega) = S_w^{-1}(\mu_1-\mu_2)
\end{equation}

Denote the projection vector as $\omega^{*} = (\omega^{*}_{D}, \omega^{*}_{T}, \omega^{*}_{C})$, for an incoming edge $e(u,v)$ of node $v$, its (unnormalized) weight $W_e$ is computed as:

\begin{equation}
    \label{eq:projection}
    W_e = \omega^{*}_{D} f_{D(e)} + \omega^{*}_{T} f_{T(e)} + \omega^{*}_{C} f_{C(e)}
\end{equation}

The applicability of \cref{eq:lda-solution} 
%in standard LDA 
requires that $S_w$ is nonsingular (\ie $S_w^{-1}$ exists).
However, this criterion may be violated quite often in our problem context, due to the large imbalance between the number of critical edges and the number of non-critical edges.
Furthermore, standard LDA only ensures that the projected values of different groups are largely separated, rather than guaranteeing which group has higher projected values, while our goal is to make critical edges have higher weights than non-critical edges.
Another limitation of standard LDA is that sometimes the projected values are negative, while we require the edge weights to be a non-negative number to model the importance w.r.t. the POI event.

%For example, if the sink node $v$ only has two incoming edges, and one of them is a critical edge, the within-cluster scatter matrix will be a zero matrix. 
%For example, it is often the case that the sink node (process entity) has one critical edge but many other non-critical edges that read system packages, and the concentration degree feature $f_{C(u, v)}$ of the non-critical edges equals zero (since system packages often have no incoming edges). In such cases, the third row and the third column of $S_w$ are all zero.

Recognizing such limitations, we \emph{extend} the standard LDA in three aspects.

\emph{(a) Handling singular $S_w$:}
When $S_w$ is singular, we select the projection vector (we normalize it first) from the following two candidates that results in a larger Fisher criterion numerator (\ie $\omega^TS_b\omega$):

\begin{itemize}[noitemsep, topsep=1pt, partopsep=1pt, listparindent=\parindent, leftmargin=*]

    \item $S_w^{+}(\mu_1-\mu_2)$. $S_w^{+}$ is the Moore-Penrose~\cite{albert1972regression} inverse of $S_w$. 
    %When $S_w$ is non-singular, $S_w^{+} = S_w^{-1}$.
    
    \item $\mu_1-\mu_2$ (\ie the direction of group mean difference)
    %difference between group means)
\end{itemize}

\emph{(b) Negating the projection vector by condition:}
To make critical edges have higher projected values, we negate the projection vector by condition.
Note that this problem is fundamentally challenging, since we do not have labels for critical edges and thus we do not know which group contains critical edges. We approach to this problem using a set of heuristics:

\begin{itemize}[noitemsep, topsep=1pt, partopsep=1pt, listparindent=\parindent, leftmargin=*]

\item If all three dimensions of the projection vector are non-positive, negate.
\item If all three dimensions of the projection vector are non-negative, do not negate.
\item Else, 
%If the projection vector has both negative dimensions and positive dimensions, 
negate by condition:

\begin{itemize}[noitemsep, topsep=1pt, partopsep=1pt, listparindent=\parindent, leftmargin=*]
    \item If only one group has seed edges and this group has a lower projected mean, negate. 
  %  If one group has seed edges and one group hasn't, negate the projection vector when necessary to make sure the cluster that has seed edges has a higher projected mean. 
    This is based on the insight that seed edges should have high weights.
    
    \item Else, if the group with a smaller size has a lower projected mean, negate.
    %If both clusters have seeds edges or neither has seed edges, negate the projection vector when necessary to make sure the cluster that has a smaller size (\ie contains fewer edges) has a higher projected mean.
    This is based on the insight that the number of critical edges is smaller than the number of non-critical edges in most cases.
\end{itemize}
\end{itemize}


\emph{(3) Scaling the projected weights:}
We scale the projected weights to the range $[0,1]$. We further add a small offset (we use 1/100 of the difference between the smallest value and the second smallest value) to each scaled weight, so that all scaled weights are positive.



\myparatight{Step 3: Local Edge Weight Normalization}
Same as local feature normalization (\cref{subsec:graph-generation}), for an edge $e(u, v)$, we \emph{locally normalize} its scaled weight by the sum of corresponding weights of all incoming edges of the sink node $v$:

\begin{equation}
    \label{eq:local-weight-normalization}
    W_e = W_e/\sum_{e' \in IncomingEdge(v)} W_{e'},
\end{equation}
%where $IncomingEdge(v)$ represents the set of incoming edges of $v$.

After normalization, the weights of all edges are in the range $(0, 1]$, and the sum of weights of all incoming edges of any node (except for nodes that do not have incoming edges) is equal to 1. 
Note that for nodes that only have one incoming edge, we skip the local clustering and projection process and directly set its final weight to 1.
This completes the Phase II by producing a weighted dependency graph with discriminative weights to distinguish critical edges from non-critical edges.
%which is amenable to the next attack investigation phase.

%Our experimental results on a wide range of attack cases (\cref{subsec:reputation-results}) clearly demonstrate the effectiveness of our discriminative local feature projection scheme.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\input{ReputationPropagation.tex}