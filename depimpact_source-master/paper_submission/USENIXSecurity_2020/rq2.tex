\input{tables/usenix/sysrepReduction.tex}
\input{tables/usenix/RQ2random.tex}
\subsubsection{Effectiveness of \tool in Graph Reduction}
\label{subsec:reductionAndMissing}

\myparatight{Evaluation Metrics}
To measure the effectiveness of graph reduction and the missing of critical edges, we compute two metrics: $M_{reduction}$ and $M_{missing}$. $M_{reduction}$ is used to measure the graph reduction rate, which is defined as:
\begin{equation}
    M_{reduction} = 1- \frac{N_{critical}}{N_{merge}}
\end{equation}
$N_{critical}$ is the number of edges in the critical component generated by \tool. $N_{merge}$ is the number of edges after preprocessing (\ie EdgeMerge). 
A good reduction method should have a large $M_{reduction}$.

$M_{missing}$ is used to measure the information loss during the graph reduction. 
Because the corresponding information is represented as edges in dependency graph, we use the critical-edge missing rate to measure the attack information loss. 
$M_{missing}$ is defined as:
\begin{equation}
    M_{missing} = \frac{N_{missing}}{N_{total}}
\end{equation}
$N_{missing}$ is the number of missing critical edges in the critical component generated by \tool. Since we have control over the test environment of these attack cases, we are able to figure out the ground truth of the attack sequences. 
We have the total number of critical edges that should be contained by the critical component, which is represented by $N_{total}$.
Without any filtering, the dependency graph constructed via performing a backward causality analysis from a POI has the $M_{missing}$ being $0.0$.



\myparatight{Entry Node Selection}
\tool chooses the top ranked entry nodes to perform forward causality analysis, and filter the edges that do not appear in the forward causality analysis. 
For this evaluation, we choose the top 3 entry nodes of each category as candidates, and thus we will have 9 nodes as candidates. 
Given these candidate nodes, we will assume the users may pick any of these 9 nodes to perform forward causality analysis for reduction.
Thus, we will compute the reduction rates by allowing users to select 1, 2, or 3 nodes among these 9 candidates, respectively, and then compute the average for these rates.

\myparatight{Comparison Approach: Monkey Approach}
To demonstrate the effectiveness of \tool's graph reduction, we compare \tool with the monkey  approach. 
For the monkey  approach, all the entry nodes are candidates and the entry nodes used to do the graph reduction are randomly selected. 
We run the monkey approach for 20 times and compute the average reduction rate.
For fairness, we will compare the results of \tool and the monkey approach with the same number of selected nodes (\ie 1, 2, and 3 nodes), respectively.


% We evaluate the graph reduction and critical-edge missing rate, when the attack investigation uses different number of candidates to do the forward reduction. 

\myparatight{Reduction Result}
% For the dependency graph reduction, if we just simply remove 99.99\% edges of dependency graph, we may have a small graph can be easily analyzed, but it is very possible that we lose all the information about attack. 
% If we only pursue to keep all the attack information, the safest reduction way is just keep all the edges. 
% However, this graph is still too large to be analyzed. 
% The ideal method is try to keep all the critical edges, at the same time remove as much irrelevant edges to POI event as possible.
\cref{tab:toolReductionAndMissing} and \cref{tab:rq2random} show the reduction results.
Columns \emph{1 Avg. Missing}, \emph{1 Avg. Reduction}, and \emph{1 Avg. \# Edge} show the average critical-edge missing rate, the average reduction rate, and the average number of edges in the graph processed by \tool and the monkey approach by only choosing 1 entry node.
Columns \emph{2 Avg. Missing}, \emph{2 Avg. Reduction}, \emph{2 Avg. \# Edge}, \emph{3 Avg. Missing}, \emph{3 Avg. Reduction}, and \emph{3 Avg. \# Edge} show the same metrics, when \tool and the monkey approach use 2 and 3 entry nodes to do the forward causality analysis, respectively. 

As expected, we can clearly see the decrease of critical-edge missing rate from 0.32 to 0.08 for \tool and from 0.28 to 0.07 for the monkey approach by using more entry nodes to do the forward causality analysis for reduction. 
\emph{By using all the top 9 entry nodes, the missing rate can reach $0.0$}.
After the reduction, \tool only has about hundreds of edges, but the graph processed by the monkey approach still has more than 30,000 edges on average. 
Compared with the monkey approach, \tool can further reduce the edges produced by the monkey approach by $99.19\%$, $99.26\%$ and $99.3\%$ using 1, 2 or 3 entry nodes, respectively.
This shows the most of the edges found by the monkey approach are non-critical edges.
Note that this improvement is not at the cost of losing critical edges for attacks. 
If \tool uses 3 entry nodes to perform forward causality analysis for reduction, the critical edge missing rate is 0.08 on average. 
At the same time, the total number of edge in the critical component is $211.06$ on average. 
For the monkey approach, this number is $30,229.19$.

% These results show that \tool is highly effective in graph reduction ($96\%$) by using 3 top ranked entry nodes, producing a dependency graph with about 200 edges on average. 
% At the same time, \tool preserves the attack information by keeping most of the critical edges ($92\%$).
These results demonstrate the superiority of \tool over the monkey approach,
which is mainly due to the better selection of the entry nodes.
For \tool, the selection of entry nodes is based on the relevance score ranking. 
For the monkey approach, this selection is a random decision. 
Based on this finding, we can conclude the effectiveness of the graph reduction highly depends on the selection of entry nodes. 


